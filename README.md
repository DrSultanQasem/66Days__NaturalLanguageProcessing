# **Journey of 66DaysofData in Natural Language Processing**

**Day1 of 66DaysOfData!**
- Natural Language Processing is a field of Linguistics, Computer Science, and Artificial Intelligence concerned with the interactions between Computers and Human language, in particular how to program computers to process and analyze large amounts of Natural Language Data. Today, I am learning NLP from very first beginning. Here, I have provided the short description of various Libraries, Dependencies and Modules required for Natural Language Processing. I have also performed the Processing of Text such as Removing the Retweet Text, Removing the Hyperlinks and Removing Hashtags. Excited about the days ahead!!

![Image](https://github.com/ThinamXx/66DaysofData__NLP/blob/master/Images/Day%201.PNG)

**Day2 of 66DaysOfData!**
- In Natural Language Processing, String Tokenization is a process where the string is split into individual words or individual parts without blanks and tabs. In the same step, the words in the string is converted into lower case. The Tokenize module from NLTK(Naural Language Toolkit) makes very easy to carry out this process. In my journey of Natural Language Processing, Today I learned about String Tokenization, Stop word and Punctuation in NLP. I have implemented TweetTokenizer and performed the process to remove stopwords and punctuation from the tokenized tweets. Excited about the days ahead!!

![Image](https://github.com/ThinamXx/66DaysofData__NLP/blob/master/Images/Day%202.PNG)

**Day3 of 66DaysOfData!**
- Stemming in Natural Language Processing: Stemming is a process of converting a word to its most general form or stem. It's basically the process of removing the suffix from a word and reduce it to its root word. It helps in reducing the size of vocabulary. In my journey of Natural Language Processing, Today I learned about Stemming in NLP which is one of the most important steps while working with text. I have presented the implementation of Porter Stemmer, Snowball Stemmer and Lancaster Stemmer. Excited for coming days!!
  - Porter Stemmer is one of the most common and gentle stemmer which is very fast but not very precise.
  - Snowball Stemmer, whose actual name is English Stemmer is more precise over large data-sets.
  - Lancaster Stemmer is very aggressive algorithm. It will hugely trim down the working data which itself has pros and cons.
  
![Image](https://github.com/ThinamXx/66DaysofData__NLP/blob/master/Images/Day%203.PNG)
  
**Day4 of 66DaysOfData!**
- Lemmatization in Natural Language Processing: Lemmatization is the process of grouping together the inflected forms of words so that they can analysed as a single item, identified by the word's lemma or a dictionary form. It is the process where individual tokens from a sentence or words are reduced to their base form. Lemmatization is much more informative than simple stemming. Lemmatization looks at the surrounding text to determine a given words's part of speech where it doesn't categorize the phrases. In my journey of Natural Language Processing, Today I learned about Lemmatization and it's simple implementation using Spacy as well NLTK. I have covered the fundamentals of NLP such as Tokenization, Stemming and Lemmatization. Excited for the days ahead!!
  
![Image](https://github.com/ThinamXx/66DaysofData__NLP/blob/master/Images/Day%204.PNG)

