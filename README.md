# **Journey of 66DaysofData in Natural Language Processing**

**Day1 of 66DaysOfData!**
- Natural Language Processing is a field of Linguistics, Computer Science, and Artificial Intelligence concerned with the interactions between Computers and Human language, in particular how to program computers to process and analyze large amounts of Natural Language Data. Today, I am learning NLP from very first beginning. Here, I have provided the short description of various Libraries, Dependencies and Modules required for Natural Language Processing. I have also performed the Processing of Text such as Removing the Retweet Text, Removing the Hyperlinks and Removing Hashtags. Excited about the days ahead!!

![Image](https://github.com/ThinamXx/66DaysofData__NLP/blob/master/Images/Day%201.PNG)

**Day2 of 66DaysOfData!**
- In Natural Language Processing, String Tokenization is a process where the string is split into individual words or individual parts without blanks and tabs. In the same step, the words in the string is converted into lower case. The Tokenize module from NLTK(Naural Language Toolkit) makes very easy to carry out this process. In my journey of Natural Language Processing, Today I learned about String Tokenization, Stop word and Punctuation in NLP. I have implemented TweetTokenizer and performed the process to remove stopwords and punctuation from the tokenized tweets. Excited about the days ahead!!

![Image](https://github.com/ThinamXx/66DaysofData__NLP/blob/master/Images/Day%202.PNG)

**Day3 of 66DaysOfData!**
- Stemming in Natural Language Processing: Stemming is a process of converting a word to its most general form or stem. It's basically the process of removing the suffix from a word and reduce it to its root word. It helps in reducing the size of vocabulary. In my journey of Natural Language Processing, Today I learned about Stemming in NLP which is one of the most important steps while working with text. I have presented the implementation of Porter Stemmer, Snowball Stemmer and Lancaster Stemmer. Excited for coming days!!
  - Porter Stemmer is one of the most common and gentle stemmer which is very fast but not very precise.
  - Snowball Stemmer, whose actual name is English Stemmer is more precise over large data-sets.
  - Lancaster Stemmer is very aggressive algorithm. It will hugely trim down the working data which itself has pros and cons.
  
![Image](https://github.com/ThinamXx/66DaysofData__NLP/blob/master/Images/Day%203.PNG)
  
**Day4 of 66DaysOfData!**
- Lemmatization in Natural Language Processing: Lemmatization is the process of grouping together the inflected forms of words so that they can analysed as a single item, identified by the word's lemma or a dictionary form. It is the process where individual tokens from a sentence or words are reduced to their base form. Lemmatization is much more informative than simple stemming. Lemmatization looks at the surrounding text to determine a given words's part of speech where it doesn't categorize the phrases. In my journey of Natural Language Processing, Today I learned about Lemmatization and it's simple implementation using Spacy as well NLTK. I have covered the fundamentals of NLP such as Tokenization, Stemming and Lemmatization. Excited for the days ahead!!
  
![Image](https://github.com/ThinamXx/66DaysofData__NLP/blob/master/Images/Day%204.PNG)

**Day5 of 66DaysOfData!**
- Natural Language Processing is a field of Linguistics, Computer Science, and Artificial Intelligence concerned with the interactions between Computers and Human language, in particular how to program computers to process and analyze large amounts of Natural Language Data. As a part of Natural Langauge Processing journey, I have started the book, [Natural Language Processing with Python](https://www.nltk.org/book/). It's really amazing and I have encountered some very basic functions but unknown to me such as concordance function, similar function, common context function and a basic dispersion plot as well. I will be using this book in my journey. Excited for the days ahead!!
- [**Natural Language Processing with Python :)**](https://www.nltk.org/book/)

**Day6 of 66DaysOfData!**
- In my journey of Natural Language Processing, Today I have explored about Gutenberg Corpus using NLTK and Python. I have also learned about various interesting challenges with proper explanation of each topic under the hood of NLP such as:
  - Word Sense Disambiguation: In NLP, Word Sense Disambiguation is an open problem concerned with identifying which sense of a word is used in a sentence. The solution to this issue impacts other Computer related writing such as discourse, improving relevance of Search Engines, Anaphora Resolution, Coherence, and Inference.
  - Pronounce Resolution
  - Generating Language Output
  - Machine Translation: Machine Translation is a sub field of Computational Linguistics that investigates the use of software to translate text or speech from one language to another.
  - Spoken Dialog System
  - Textual Entailment

![Image](https://github.com/ThinamXx/66DaysofData__NLP/blob/master/Images/Day%206.PNG)

**Day7 of 66DaysOfData!**
- In my journey of Natural Language Processing, Today I have learned about different Text Corpora and Basic Corpus Functionality defined in NLTK. I have also learned about Loading own Corpus, Plotting and Tabulating Distributions and Generating Random Text with Bigrams. The topics I have covered are summarized below:
  - Web and Chat Text
  - Brown Corpus
  - Reuters Corpus
  - Inaugural Address Corpus
  - Annotated Text Corpora
  - Corpora in Other Languages
  - Conditional Frequency Distributions

![Image](https://github.com/ThinamXx/66DaysofData__NLP/blob/master/Images/Day%207.PNG)

**Day8 of 66DaysOfData!**
- In my journey of Natural Language Processing, Today I am learning about Processing Raw Text in Natural Language Processing. Basically, I have completed processing the Text from Electronic Books and from HTML documents. Apart from that, I have learned about WordNet. The topics I have covered in WordNet are:
  - The WordNet Hierarchy and
  - Semantic Similarity: Semantic Similarity is a metric defined over a set of Documents or Terms where the idea of distance between items is based on the likeness of their meaning or semantic content as opposed to Lexicographical Similarity. Example:"Road" and "Driving".

![Image](https://github.com/ThinamXx/66DaysofData__NLP/blob/master/Images/Day%208.PNG)

**Day9 of 66DaysOfData!**
- In my journey of Natural Language Processing, Today I have learned about Text Processing with Unicode and Regular Expressions for Detecting Word Patterns with proper explanations. Within a Program, we can manipulate Unicode just like normal strings. Unicode are encoded as stream of bytes. Examples: ASCII, Latin-2, UTF-8. I have also implemented the Regular Expressions for Detecting the Word Patterns using basic meta characters such as:
  - Dollar sign ( $ ) matches the characters of the end of word.
  - Dot symbol ( . ) matches any single character.
  - Caret symbol ( ^ ) matches the characters of the start of word.
  - Question mark ( ? ) specifies the previous character is optional.
  - Plus sign ( + ) means one or more instances of the preceding item.
  - Sign ( * ) means zero or more instances of the preceding item.
  - Backslash ( \ ) means following character is deprived.
  - Pipe symbol ( | ) specifies the choice between left and right.

![Image](https://github.com/ThinamXx/66DaysofData__NLP/blob/master/Images/Day%209.PNG)

**Day10 of 66DaysOfData!**
- In my journey of Natural Language Processing, Today I have learned about the various useful applications of Regular Expressions such as Finding Word Stems, Regular Expressions for Tokenizing Text, Normalizing Text such as Stemmers and Lemmatization. I have also read about the issues of Tokenization: Tokenization turns out to be far more difficult task than one might have expected. No single solution works well accross the board. Another issue of Tokenization is the presence of contractions as well. Example: didn't.

![Image](https://github.com/ThinamXx/66DaysofData__NLP/blob/master/Images/Day%2010.PNG)

**Day11 of 66DaysOfData!**
- In my journey of Natural Language Processing, Today I have completed all the preliminaries process or techniques required in Natural Language Processing included in the book, "Natural Language Processing with Python". I have completed the topics such as Tokenization, Stemming, Lemmatization and Text Processing with Regular Expressions. Apart from that, I have also worked with various Text Corpora such as Brown Corpus, Inaugural Address Corpus, Reuters Corpus and so on. And I have completed first 110 pages of the book, "Natural Language Processing with Python".

![Image](https://github.com/ThinamXx/66DaysofData__NLP/blob/master/Images/Day%2011%20a.PNG)

- I have plotted a simple bar plot using various categories of Brown Corpus presenting the Frequency Distribution of various words appearing inside the Corpus. I hope you can gain insights from the plot as well. Excited about the days ahead !!

![Image](https://github.com/ThinamXx/66DaysofData__NLP/blob/master/Images/Day%2011.PNG)

**Day12 of 66DaysOfData!**
- Part of Speech Tagging: The process of classifying words into their "Parts of Speech" and Labeling them accordingly is known as Part of Speech Tagging which is also known as POS tagging or simply Tagging. The collections of tags used for a particular task is known as Tagset. In my journey of Natural Language Processing, Today I have learned about Automatic Tagging such as Default Tagger, Regular Expression Tagger and Lookup Tagger along with N-Gram Tagging such as Unigram Tagger, Bigram Tagger and so on. I have also learned about Combining Taggers using backoff parameter. I hope you can gain insights about the Implementation of Tagging. Excited about the days to come !!

![Image](https://github.com/ThinamXx/66DaysofData__NLP/blob/master/Images/Day%2012.PNG)

**Day13 of 66DaysOfData!**
- Supervised Classification: Classification is the process of choosing the correct class label for a given input. The Classification is said to be Supervised Classification if it is built based on training corpora containing the correct label for each input. One common example of Classification is deciding whether an Email is spam or not. In my journey of Natural Language Processing, Today I have learned about Supervised Classification. I have covered the topics such as Choosing Right Features, Document Classification, Gender Identification, Part of Speech Tagging using Naive Bayes Classifier and Decision Trees Classifier under the hood of Supervised Classification of NLP. I have presented the basic Implementation of Naive Bayes Classifier in Document Classification and I hope you can gain insights from it as well. Excited about the days ahead!!

![Image](https://github.com/ThinamXx/66DaysofData__NLP/blob/master/Images/Day%2013.PNG)

**Day14 of 66DaysOfData!**
- In my journey of Natural Language Processing, Today I have learned about Sequence Classification, Sentence Segmentation and various Evaluation methods under the hood of Natural Language Processing. I have covered the fundamental topics such as Test Data, Accuracy measure, Precision and Recall, Confusion Matrices, Cross validation, Decision Trees and Naive Bayes with proper implementations which are so helpful for my understanding. I have been in this journey for 2 weeks and I have covered all the fundamentals which are so relevant in Natural Language Processing. I had been following the "Natural Language Processing with Python" book and it really helps me a lot. Now, I will be focusing more on Implementations so that I will be following the course of Fastai on Natural Language Processing. I have implemented the Naive Bayes Classifier in Text Corpus and I hope you can gain insight about the Implementation of Naive Bayes. Excited about the days ahead!!

![Image](https://github.com/ThinamXx/66DaysofData__NLP/blob/master/Images/Day%2014%20a.PNG)

**Day15 of 66DaysOfData!**
- Singular Value Decomposition (SVD): The words that appear most frequently in one topic would appear less frequently in the other, otherwise that word wouldn't make a good choice to separate out the two topics. Therefore, the topics are Orthogonal. The SVD algorithm factorizes a matrix into one matrix with orthogonal columns and one with orthogonal rows along with diagonal matrix which contains the relative importance of each factor.
- NonNegative Matrix Factorization (NMF): Non Negative Matrix Factorization (NMF) is a factorization or constrain of non negative dataset. NMF is non exact factorization that factors into one short positive matrix.
- Topic Frequency Inverse Document Frequency (TFIDF): TFIDF is a way to normalize the term counts by taking into account how often they appear in a document and how long the document is and how common or rare the document is.
- In my journey of Natural Language Processing, Today I have learned and implemented SVD, NMF and TFIDF in Topic Modeling project. I have captured just the overview of the implementations here. I hope you can gain insights.
- [**Topic Modeling with SVD and NMF**](https://github.com/ThinamXx/NaturalLanguageProcessing_NLP/blob/master/Topic%20Modeling%20with%20SVD%20and%20NMF.ipynb)

![Image](https://github.com/ThinamXx/66DaysofData__NLP/blob/master/Images/Day%2015.PNG)
![Image](https://github.com/ThinamXx/66DaysofData__NLP/blob/master/Images/Day%2015%20a.PNG)

**Day16 of 66DaysOfData!**
- In my journey of Natural Language Processing, Today I have learned the Implementations of NLP from Fastai course which has been published recently. As the Fastai course primarily focuses on the Coding part and follows the top down aspect of Learning and Teaching. It's bit complicated to learn than other courses. Fastai's API is really amazing and powerful as well. I learned the basic steps of NLP with Fastai such as Word Tokenization, Subword Tokenization, Numericalization, and Preparing TextBlock and DataBlock. I am currently working on Sentiment Analysis of IMDB reviews using Fastai. I have shared the Implementations of the Word Tokenization, Subword Tokenization, Numericalization and process to prepare the TextBlock and DataBlock with Fastai here. I hope you can gain insights about the Implementation of Fastai from here. Excited about the days ahead!!

![Image](https://github.com/ThinamXx/66DaysofData__NLP/blob/master/Images/Day%2016.PNG)

**Day17 of 66DaysOfData!**
- In my journey of Natural Language Processing, Today I have learned about the Implementation of Fastai in preparing a Sentiment Classifier Model. I have prepared a Model using Fastai API which can classify Sentiment of Internet Movie Database reviews i.e. classifying the Positive or Negative Sentiment. Fastai's API is really powerful and effective so that the Model can classify the Sentiment of Internet Movie Database reviews with above 90% accuracy in just few lines of code. I have learned about Word Tokenization, Subword Tokenization, Numericalization, TextBlock and DataBlock API and Training the Classifier Model using Fastai.I have presented the snapshot of the Implementation of Fastai API in preparing the Language Model and Training the Model. I have also presented the Implementation of Fastai API in preparing the Classifier Model using the Language Model and also the concept of unfreezing the Model. I hope you can gain insights from here as well. This snapshot is the continuation of yesterday's snapshot. Excited about the days ahead!!
- [**Sentiment Classification of Internet Movie Database reviews**](https://github.com/ThinamXx/InternetMovieDatabase__NLP/blob/master/IMDB.ipynb)

![Image](https://github.com/ThinamXx/66DaysofData__NLP/blob/master/Images/Day%2017.PNG)

**Day18 of 66DaysOfData!**
- In my journey of Natural Language Processing, Today I have read a topic of Natural Language Processing in the book [**Dive into Deep Learning**](https://d2l.ai/) by Aston Zhang. Here, I have covered the topics such as Text Processing, Machine Translation, Natural Language Processing pre training and Word Embedding. The information and explanations were great and the code implementation is in MXNET. I am not quite familiar with MXNET framework so I have just presented a small snapshot here. Apart from that, Today I have read a very small part of the book [**Natural Language Processing in Action**](https://g.co/kgs/U82Sem) by Hobson Lane. I am so much thankful to Anthony Camarillo for sharing this book with me. And I will give continuation with this book [**Natural Language Processing in Action**](https://g.co/kgs/U82Sem). Excited about the days ahead!!

![Image](https://github.com/ThinamXx/66DaysofData__NLP/blob/master/Images/Day%2018.PNG)

**Day19 of 66DaysOfCode!**
- In my journey of Natural Language Processing, Today I have read and implemented the first chapter of the book, [Natural Language Processing in Action](https://g.co/kgs/U82Sem). In this chapter, I have covered the topics such as Natural Language Processing and the Magic, Regular Expressions, Finite State Machine (FSM) concept, Word order and Grammar, simple NLP Chatbot pipeline and Natural Language IQ. I have presented the simple Chatbot using Regular Expressions and Finite State Machine (FSM) concept. Basically, I will be working on much advanced Chatbots using Neural Networks in coming days. I hope you will also google out the FSM concept in NLP and also the Implementation of Regular Expressions in FSM from here. Excited about the days ahead!!

![Image](https://github.com/ThinamXx/66DaysofData__NLP/blob/master/Images/Day%2019.PNG)

**Day20 of 66DaysOfData!**
- In my journey of Natural Language Processing, Today I have read a couple of topics under the hood of Natural Language Processing such as Tokenized Phrase, Dot Product in NLP, Bag of Words and Overlapping, Token improvement with Regex which is apart from Regular Expressions, Contractions, Extending Vocabulary with NGrams. I have also read and implemented the Regex Tokenizer, Tree Bank Tokenizer and Casual Tokenizer. Actually I am continuing my learning journey with a book [Natural Language Processing in Action](https://g.co/kgs/U82Sem) and lots of the preprocessing concepts which I have already read are coming along my way. I prefer to go through the concepts again because I don't want to skip any topics from this book. Although the concepts might match along the way I won't repeat the same implementation in any of my Snapshots. I have presented the Implementation of Regex Tokenizer, Tree Bank Tokenizer, Casual Tokenizer and NGrams here in the Snapshots. These Tokenization steps are more better than Traditional Tokenization steps using Regular Expressions. I hope you will spend some time to learn about these Tokenizers. Excited about the days ahead!!

![Image](https://github.com/ThinamXx/66DaysofData__NLP/blob/master/Images/Day%2020.PNG)

**Day21 of 66DaysOfData!**
- In my journey of Natural Language Processing, Today I have read and implemented about Stopwords, Stemming, Lemmatization and Sentiment Analysis using VADER Approach which is the Algorithm composed by human and also the Machine Learning Approach with the help of Naive Bayes Classifier. I have completed the first 2 chapters of the book, [Natural Language Processing in Action](https://g.co/kgs/U82Sem) and it is helping me a lot along my journey. I have presented the Implementation of VADER Approach in Sentiment Analysis along with Naive Bayes Classifier and I have also included the Implementation of Casual Tokenizer in the Movies Dataset. I hope you will gain insights about the Implementation of VADER Approach and Naive Bayes in Sentiment Analysis. Actually, VADER Approach is not as efficient as Machine Learning Approach such as Naive Bayes Classifier. I hope you will spend some time in learning about Naive Bayes in Sentiment Analysis along with VADER Approach. Excited about the days to come!!

![Image](https://github.com/ThinamXx/66DaysofData__NLP/blob/master/Images/Day%2021.PNG)

**Day22 of 66DaysOfData!**
- In my journey of Natural Language Processing, Today I have read and Implemented about Bag of Words, Vectorizing concept, Vector Spaces, Cosine Similarity, Zipf's Law and Inverse Frequency concept, Text Modeling, TFIDF, Relevance Ranking and Okapi BM25 concept. I have completed the first three chapters of the book [Natural Language Processing in Action](https://g.co/kgs/U82Sem) and this chapter primarily focuses on the concept of Vectorizing the Tokens which are obtained after Tokenization using TFIDF Vectorizer. Text Vectorization is the process of converting Text into Numerical representation. I have also read and Implemented the concept of Cosine Similarity under the hood of Natural Language Processing. I have presented the Implementation of TFIDF Vectorizer and also the process of Tokenizing the Text Documents and removing the Stopwords. I have also Implemented the Cosine Similarity using Numpy and pure Python as well in this Snapshot. I hope you will gain insights about Text Vectorization and Tokenization from here. Excited about the days to come!!

![Image](https://github.com/ThinamXx/66DaysofData__NLP/blob/master/Images/Day%2022.PNG)

**Day23 of 66DaysOfData!**
- In my journey of Natural Language Processing, Today I have started learning about Semantic Analysis. I have read and implemented about Topic Vectors, Algorithms for Scoring Topics and Semantic Analysis such as Latent Semantic Analysis **LSA**, Linear Discriminant Analysis **LDA** and Latent Dirichlet Allocation **LDIA**. Today, I primarily focused on reading and Implementing about Linear Discriminant Analysis **LDA**. LDA is one of the most straight forward and fast Dimension Reduction and Classification Models. In Natural Language Processing, Semantic Analysis is the process of relating the Syntactic structures from the levels of Phrases, Clauses, Sentences and Paragraphs to the level of the writing as a whole and to their Language dependent meanings. I have presented the Implementation of LDA's working Principal which states that Computing the centroid of TFIDF vectors for each side of the binary Class here in the Snapshot. I hope you will gain insights about Implementation of LDA Classifier and creating NLP Pipeline of Tokenizer and Vectorizer from here. And I hope you will spend some time in learning about Semantic Analysis as well. Excited about the days ahead !!

![Image](https://github.com/ThinamXx/66DaysofData__NLP/blob/master/Images/Day%2023.PNG)

**Day24 of 66DaysOfData!**
- In my journey of Natural Language Processing, Today I have read and Implemented Latent Semantic Analysis "LSA", Singular Value Decomposition "SVD", Principal Component Analysis "PCA", Truncated SVD and Latent Dirichlet Allocation "LDIA". I have primarily focused on reading and Implementing LSA and LDIA for Semantic Analysis. LSA works well with Normalized TFIDF Vectors whereas LDIA works well with raw Bag of Words "BOW" Count Vectors. Semantic Analysis is the process of relating the Syntactic structures from the levels of Phrases, Clauses, Sentences and Paragraphs to the level of the writing as a whole and to their Language dependent meanings. I have presented the Implementation of Linear Discriminant Analysis "LDA" while working with TFIDF Vectors and BOW Count Vectors here in the Snapshot. I hope you will gain insights about the Implementation of LDA Classifier along with LDIA Topic Vectors and BOW count Vectors. Incase you want to see my Notebook, I have presented the overall Implementation of Semantic Analysis with LSA, LDA and LDIA with proper Documentation here: [**Semantic Analysis**](https://github.com/ThinamXx/SemanticAnalysis__NLP.git). Excited about the days to come!!

![Image](https://github.com/ThinamXx/66Days__NaturalLanguageProcessing/blob/master/Images/Day%2024.PNG)

**Day25 of 66DaysOfData!**
- In my journey of Natural Language Processing, Today I have started learning and Implementing Neural Networks and Deep Learning for Natural Language Processing. I have completed the Implementation of LSA and LDIA in Semantic Analysis along with LDA. I have read the topics such as Neural Networks and Perceptrons, Gradients, Local and Global Minimum and Backpropagation under the hood of Neural Networks and Deep Learning. Actually, I have primarily focused on reading the topics needed to understand the Neural Networks and Deep Learning rather than Implementing the concepts. I have presented the Simple workflow of Neural Networks using Keras API. I will be Implementing the Keras API in Natural Language Processing from today. I hope you will also spend some time to learn the basic topics which I have mentioned above to understand the Neural Networks and Deep Learning. Excited to learn and Implement Neural Networks for NLP in coming days!!

![Image](https://github.com/ThinamXx/66Days__NaturalLanguageProcessing/blob/master/Images/Day%2025.PNG)

**Day26 of 66DaysOfData!**
- In my journey of Natural Language Processing, Today I have read and Implemented about Word Vectors, Softmax Function, Negative Sampling, Document Similarity with Doc2Vec and Google's Word2vec, GloVe and Facebook's FastText Models which were pretrained on Billions of Text Data. I have primarily focused on learning and Implementing the Word2vec pretrained Model today. I am continuing my learning journey along with the book, [**Natural Language Processing in Action**](https://g.co/kgs/U82Sem). Word2vec is a Model for Natural Language Processing. The Word2vec Algorithm uses a Neural Network Model to learn Word associations from a large corpus of text. Once Word2vec Model is trained, It can detect Synonymous words or suggest additional words for a partial sentence. Word2vec is a group of related Models that are used to produce Word Embeddings. I have presented the Implementation to access the Google's Word2vec pretrained Model and it's basic Functions and the process to create own Domain Specific Word2vec Model. I have also presented the Implementation of Doc2vec Model here in the Snapshot. I hope you will also spend so time to learn about Word2vec pretrained Model. Excited about the days to come!!

![Image](https://github.com/ThinamXx/66Days__NaturalLanguageProcessing/blob/master/Images/Day%2026.PNG)

**Day27 of 66DaysOfData!**
- In my journey of Natural Language Processing, Today I have read and Implemented about Convolutional Neural Network for Natural Language Processing. I have covered the topics such as CNN building blocks, Step size or Stride, Filter Composition, Padding, Convolutional Pipeline, Learning, Narrow Windows and Implementing the Keras API under the hood of Convolutional Neural Network for NLP. I have started working on the Sentiment Analysis of Large Movie Review Dataset which was compiled for the 2011 paper "Learning Word Vectors for Sentiment Analysis". Since, It is a very large Dataset, I have used just the subset of the Dataset. I will be Implementing CNN for this Project. I have presented the basic Implementations of approaching the Dataset such as Importing the Dependencies, Processing the Dataset with Tokenization and Google News pretrained Model Vectorization and Splitting the Dataset into Training set and Test set. I have presented the overall Implementation of above mentioned approaches with proper Documentation here: [**Sentiment Analysis of Large Movie Dataset**](https://github.com/ThinamXx/ConvolutionalNeuralNetwork__andNLP.git)

![Image](https://github.com/ThinamXx/66Days__NaturalLanguageProcessing/blob/master/Images/Day%2027b.PNG)

![Image](https://github.com/ThinamXx/66Days__NaturalLanguageProcessing/blob/master/Images/Day%2027a.PNG)

**Day28 of 66DaysOfData!**
- In my journey of Natural Language Processing, Today I have read and Implemented about Convolutional Neural Network for Natural Language Processing. I have covered the topics such as Convolutional Neural Network Architecture, Pooling, Dropout, CNN parameters, Optimization and Training CNN Model under the hood of Convolutional Neural Network for Natural Language Processing. I have presented the overall Implementation of Convolutional Neural Network here in the Snapshot. I have also presented the short info of all the parameters mentioned in the CNN Model and the process of Compiling and Training the Model as well. I hope you will gain insights about the Implementation of CNN Model in Sentiment Analysis. Actually, It is the continuation of yesterday's Snapshots. I hope you will also spend some time working on it. I have completed working on the Sentiment Analysis of Large Movie Review Dataset. I have prepared a Model using Convolutional Neural Network which can classify the Sentiment of Text Data. The Notebook with complete Documentation is here: [**Sentiment Analysis of Large Movie Dataset**](https://github.com/ThinamXx/ConvolutionalNeuralNetwork__andNLP.git). Excited about the days to come!!

![Image](https://github.com/ThinamXx/66Days__NaturalLanguageProcessing/blob/master/Images/Day%2028.PNG)

**Day29 of 66DaysOfData!**
- In my journey of Natural Language Processing, Today I have read and Implemented about Simple Recurrent Neural Network as well as Bidirectional Recurrent Neural Network. Here, I have covered the topics such as Backpropagation through Time, Hyperparameters, Statefulness, Bidirectional Networks and various other topics similar to Convolutional Neural Network mentioned in the previous Snapshot. I have Implemented the Recurrent Neural Network in the same Large Movie Review Dataset to predict the Sentiment of Text Data. I have presented the overall Implementation of RNN Model here in the Snapshot. I have also presented the short info of all the Parameters and codes mentioned here in the Model. I hope you will gain some insights about the Implementation of RNN Model in Sentiment Analysis. Although the Implementation of RNN Model is not so appreciable, I hope you will spend some time understanding the working principle of RNN and working on the same. I have completed working on the Sentiment Analysis of Large Movie Review Dataset using Simple RNN as well as Bidirectional RNN. The Notebook with complete Documentation is here: [**Sentiment Analysis of Large Movie Dataset**](https://github.com/ThinamXx/ConvolutionalNeuralNetwork__andNLP.git). Excited about the days to come!!

![Image](https://github.com/ThinamXx/66Days__NaturalLanguageProcessing/blob/master/Images/Day%2029.PNG)

**Day30 of 66DaysOfData!**
- In my journey of Natural Language Processing, Today I have read and Implemented about Long Short Term Memory or LSTM. I have covered the topics such as LSTM working principle, Backpropagation through time, Keras API and various other topics similar to CNN and RNN as mentioned in the previous Snapshots under the hood of NLP. Actually, I have primarily focused on Implementing the LSTM Model in the same Large Movie Review Dataset to compare the effectiveness of CNN, RNN and LSTM on Sentiment Analysis of Text Data. Long Short Term Memory or LSTM is an Artificial Recurrent Neural Network or RNN architecture used in the field of Deep Learning. Unlike standard Feedforward Neural Networks, LSTM has Feedback connections. I have presented the Simple Implementation of Long Short Term Memory or LSTM Model in the same Large Movie Review Dataset. I have also presented the short info of all the Parameters and codes mentioned here in the Model. I hope you will gain some insights about the Implementation of LSTM Model in Sentiment Analysis. I have completed working on the same with LSTM Model. The Notebook with complete Documentation is here: [**Sentiment Analysis of Large Movie Dataset**](https://github.com/ThinamXx/ConvolutionalNeuralNetwork__andNLP.git). Excited about the days ahead!!

![Image](https://github.com/ThinamXx/66Days__NaturalLanguageProcessing/blob/master/Images/Day%2030.PNG)

**Day31 of 66DaysOfData!**
- Long Short Term Memory or LSTM: Long Short Term Memory or LSTM is an Artificial Recurrent Neural Network or RNN architecture used in the field of Deep Learning. Unlike standard Feedforward Neural Networks, LSTM has Feedback connections. It can not only process single data points, but also entire sequences of data such as Speech or Video. LSTM is applicable for Text Generation as well. Today I have read and Implemented about Generating the Text using Long Short Term Memory or LSTM. I have prepared a Model using LSTM which generates the Text similar to William Shakespeare's writing. I have used the Gutenberg Corpus which contains the 3 plays of Shakespeare to train the Neural Network. I have presented the Implementation of LSTM Model as well as the Implementation of Keras API in Text Generation here in the Snapshot. I have also presented the Snapshot of Generated Text with the help of LSTM Model. I hope you will gain some insights and you will also spend some time working on the same. I have completed working on Generating Text with the help of LSTM Model. The Notebook with complete Documentation is here: [**Generating Text with LSTM**](https://github.com/ThinamXx/NeuralNetwork__SentimentAnalysis/blob/master/Generating%20Text%20with%20LSTM.ipynb). I am excited about the days to come !!

![Image](https://github.com/ThinamXx/66Days__NaturalLanguageProcessing/blob/master/Images/31a.PNG)

![Image](https://github.com/ThinamXx/66Days__NaturalLanguageProcessing/blob/master/Images/31b.PNG)

**Day32 of 66DaysOfData!**
- In my journey of Natural Language Processing, Today I have read and Implemented the topics such as Encoder and Decoder Architecture in Sequence to sequence Models, Thought Vector, Decoding Thought using LSTM, Sequence Encoder and Keras Functional API in assembling Sequence to Sequence Pipeline under the hood of Natural Language Processing. I have started working on building a Chatbot using Sequence to sequence Neural Networks and Keras Functional API. I have presented the simple Implementation of processing the Text Corpus and few steps to make the Text Data ready to train the Sequence to sequence Chatbot here in the Snapshot. I will be using the Cornell Dialog Dataset for training the Chatbot. I hope you will gain some insights from it and you will also spend some time working on the same. [**Chatbot with Sequence to sequence Networks**](https://github.com/ThinamXx/Chatbot.git). I am so much excited about the days to come!!

![Image](https://github.com/ThinamXx/66Days__NaturalLanguageProcessing/blob/master/Images/Day%2032.PNG)

**Day33 of 66DaysOfData!**
- Sequence to Sequence Model: Sequence to Sequence Neural Networks can be built with a modular and reusable Encoder and Decoder Architecture. The Encoder Model generates a Thought Vector which is a Dense and fixed Dimension Vector representation of the Data. The Decoder Model use Thought Vectors to generate Output Sequences. In my journey of Natural Language Processing, Today I have read and Implemented various topics under Sequence to Sequence Networks. I have continued working on the Chatbot using Sequence to Sequence Learning. I have used the Keras Functional API and Cornell Dialog Dataset for Training the Model. I have presented the Implementation of Thought Encoder and Thought Decoder using Keras Functional API here in the Snapshot. I have also presented the techniques for Training the Model and Generating the Response Sequences here. I hope you will gain some insights from it and you will also spend some time working on the same. [**Chatbot with Sequence to sequence Networks**](https://github.com/ThinamXx/Chatbot.git). I am excited about the days to come !!

![Image](https://github.com/ThinamXx/66Days__NaturalLanguageProcessing/blob/master/Images/Day%2033.PNG)

**Day34 of 66DaysOfData!**
- In my Journey of Natural Language Processing, Today I have read about Sentence Segmentation, Named Entity Recognition, Understanding Chatbot Approaches and few NLP Pipelines under the hood of Natural Language Processing. I have also started reading the book **Natural Language Processing with PyTorch**. Actually, I had never worked with **PyTorch** and never found any particular reason to start with **PyTorch**. But, Today I got motivated to start **Natural Language Processing with PyTorch**. I will be reading and Implementing **Natural Language Processing with PyTorch** from today. I have presented the simple Implementation of AIML Bot and the Vectorization concept here in the Snapshot. I am fond of revisiting the small concepts again and again so that I won't get stuck while Implementing in real problems. I am so excited to start **Natural Language Processing with PyTorch** in coming days!!

![Image](https://github.com/ThinamXx/66Days__NaturalLanguageProcessing/blob/master/Images/Day%2034a.PNG)

![Image](https://github.com/ThinamXx/66Days__NaturalLanguageProcessing/blob/master/Images/Day%2034b.PNG)

**Day35 of 66DaysOfData!**
- In my journey of Natural Language Processing, Today I have read and Implemented The Supervised Learning Paradigm, Computational Graphs, PyTorch Basics and various NLP fundamentals such as Tokenization, Lemmatization, Tagging and Semantics. I have also read and Implemented about Activation Functions such as Sigmoid, Tanh, ReLU, Softmax and Loss Functions such as Mean Squared Error and Cross Entropy under the hood of **Natural Language Processing with PyTorch**. I have presented the Implementation of PyTorch in various Activation Functions and Loss Functions along with few preliminaries for Understanding and working with PyTorch here in the Snapshots. I hope you will get some insights about the Implementations of PyTorch and I hope that you will also spend some time working on the same and get ready to start the Implementation of PyTorch in Natural Language Processing in coming days!!

![Image](https://github.com/ThinamXx/66Days__NaturalLanguageProcessing/blob/master/Images/Day%2035a.PNG)

![Image](https://github.com/ThinamXx/66Days__NaturalLanguageProcessing/blob/master/Images/Day%2035b.PNG)

**Day36 of 66DaysOfData!**
- Regularization: In Machine Learning, Regularization is the process of adding Information in order to solve a well posed problems or to prevent Overfitting. In my journey of Natural Language Processing, Today I have read and Implemented about Model Selection approaches, Choosing Loss Functions, Choosing Optimizers, Gradient Based Supervised Learning, Evaluation Metrics, Regularization and Early Stopping under the hood of **Natural Language Processing with PyTorch**. I have started working on the YELP Reviews Dataset and the Neural Networks will be Implemented using PyTorch. I have presented some Data Preprocessing Techniques which I have Implemented while working with YELP Reviews Dataset here in the Snapshot. I haven't completed working on the same. I hope you will also spend some time working on the same and I am so much excited to Implement PyTorch in Natural Language Processing in coming days. [**YELP Reviews Sentiment Analysis**](https://github.com/ThinamXx/YELPReviews__PyTorch.git).

![Image](https://github.com/ThinamXx/66Days__NaturalLanguageProcessing/blob/master/Images/Day%2036.PNG)

**Day37 of 66DaysOfData!**
- In my journey of Natural Language Processing, Today I have read and Implemented about PyTorch's Data Representation, The Vocabulary Class, The Vectorizer Class and The DataLoader Class under the hood of **Natural Language Processing with PyTorch**. I am continuing my journey along with the book **Natural Language Processing with PyTorch**. 
  - **The Vocabulary Class**
    - The Vocabulary Class not only manages the Bijection i.e Allowing user to add new Tokens and have the Index auto increment but also handles the special token called UNK which stands for Unknown. By using the UNK Token, It will be easy to handle Tokens at Test time that were never seen in Training Instance.
  - **The Vectorizer Class**
    - The second stage of going from a Text Dataset to a vectorized minibatch is to iterate through the Tokens of an Input Data Point and convert each Token to its Integer form. The result of this iteration should be a Vector. Because this Vector will be combined with Vectors from other Data points, there is Constraint that the Vectors produced by the Vectorizer should always have the same length.
  - **The DataLoader Class**
    - The Final step of Text to Vectorized minibatch pipeline is to actually group the Vectorized Datapoints. Because grouping into mini batches is a viatal part of Training the Neural Networks, PyTorch provides a built in class called DataLoader for coordinating the Process.
- I have presented the Implementation of Dataset Class using PyTorch here in the Snapshot. I have been working on the Implementation of Dataset Class, Vectorizer Class and DataLoader Class and I feel quite overwhelmed with the complexity of PyTorch because I am not familiar with PyTorch Framework. But I am keeping track on every lines of the Code. I hope you will also spend some time working on the same and I am so much excited about the coming day. [**YELP Reviews Sentiment Analysis**](https://github.com/ThinamXx/YELPReviews__PyTorch.git).

![Image](https://github.com/ThinamXx/66Days__NaturalLanguageProcessing/blob/master/Images/Day%2037a.PNG)

![Image](https://github.com/ThinamXx/66Days__NaturalLanguageProcessing/blob/master/Images/Day%2037b.PNG)

**Day38 of 66DaysOfData!**
- In my journey of Natural Language Processing, Today I have read and Implemented about Perceptron Classifier, The Training Routine Class using PyTorch's Implementation, The Training Loop, Evaluation, Inference and Inspection of the Model prepared using PyTorch under the hood of "Natural Language Processing with PyTorch". I am continuing my journey along with the book, **Natural Language Processing with PyTorch**. Today, I have continued working with YELP Review Dataset for Sentiment Analysis using PyTorch. I have presented the simple Implementation of PyTorch in Training the Classifier Model along with the process of Instantiating the Dataset, Model, Loss, Optimizer and Training State here in the Snapshots. Actually, It is the continuation of yesterday's Snapshot. I hope you will gain some insights about the Implementation of PyTorch in Natural Lanuage Processing from here. And I hope you will also spend some time working on the same. [**YELP Reviews Sentiment Analysis**](https://github.com/ThinamXx/YELPReviews__PyTorch.git). I am so much excited about the days to come !!

![Image](https://github.com/ThinamXx/66Days__NaturalLanguageProcessing/blob/master/Images/Day%2038b.PNG)

![Image](https://github.com/ThinamXx/66Days__NaturalLanguageProcessing/blob/master/Images/Day%2038a.PNG)

**Day39 of 66DaysOfData!**
- Long Short Term Memory (LSTM): Long Short Term Memory or LSTM is an Artificial Recurrent Neural Network or RNN architecture used in the field of Deep Learning. LSTM has Feedback connections. It can not only process single data points, but also entire sequences of data such as Speech or Video. In my journey of Natural Language Processing, Today I have started working on new Text Dataset i.e Amazon Electronics Reviews Dataset for Analysis using only TensorFlow and TensorBoard. I will be working on the same until I finish it completely. Apart from that, Today I have watched some videos on YouTube and read some Notebooks in Kaggle under the hood of Natural Language Processing. I have presented the overall Implementation of TensorFlow and TensorBoard in Processing the Data such as Tokenization and Encoding and the techniques for preparing LSTM Model here in the Snapshots. I hope you will gain some insights and you will also spend some time working on the same. [**Amazon Reviews Analysis**](https://github.com/ThinamXx/AmazonReviews__Analysis.git). Excited about the days to come!!

![Image](https://github.com/ThinamXx/66Days__NaturalLanguageProcessing/blob/master/Images/Day%2039a.PNG)

![Image](https://github.com/ThinamXx/66Days__NaturalLanguageProcessing/blob/master/Images/Day%2039b.PNG)

**Day40 of 66DaysOfData!**
- In my journey of Natural Language Processing, Today I have continued working on yesterday's Notebook which was of Text Dataset i.e Amazon Electronics Reviews Dataset for Analysis using only TensorFlow and TensorBoard. Actually, I spent most of my time in Training the Model and I tried to optimize the Model to increase the speed of Training but still the Training procedure took most part of my time using GPU as well. Apart from that, Today I have spent some time working on Greedy Programming Algorithms. Actually I got a chance to interview from one of the Tech Giants as well. I had prepared a Model using LSTM which was trained on Amazon Reviews Dataset. This Snapshot is the continuation of yesterday's Snapshots. So, I have presented some basic workflow of Model Evaluation and deploying the Trained Model on unseen Text Data for Analysis. I have also presented the simple technique of Data Visualization for evaluating the Model here in the Snapshot. I hope you will also spend some time working on the same Dataset. [**Amazon Reviews Analysis**](https://github.com/ThinamXx/AmazonReviews__Analysis.git). I am so much excited about the days to come !!

![Image](https://github.com/ThinamXx/66Days__NaturalLanguageProcessing/blob/master/Images/Day%2040a.PNG)

**Day41 of 66DaysOfData!**
- In my journey of Natural Language Processing, Today I have read and Implemented about Feed Forward Networks for Natural Language Processing using PyTorch. I have covered the topics such as The Multi Layer Perceptron (MLP), Simple XOR Functions with MLP using PyTorch and Softmax Activation Function under the hood of **Natural Language Processing with PyTorch**. I have started working in Surname Classification Model Inferring Demographic Information which has applications from Product Recommendations to ensuring fair outcomes for users across different Demographics. I will be using PyTorch for building the Classifier Model. I have presented the Implementation of PyTorch in building the simple MLP Class here in the Snapshots. I have also presented the techniques for processing the raw Dataset for Surname Classification Project using PyTorch. I hope you will gain some insights and you will also spend some time learning about MLP and working on the same and get ready for building the Model. Excited about the days ahead !!

- [**Surname Classification with Demographics: PyTorch**](https://github.com/ThinamXx/SurnameClassification__PyTorch.git)

![Image](https://github.com/ThinamXx/66Days__NaturalLanguageProcessing/blob/master/Images/Day%2041a.PNG)

![Image](https://github.com/ThinamXx/66Days__NaturalLanguageProcessing/blob/master/Images/Day%2041b.PNG)
