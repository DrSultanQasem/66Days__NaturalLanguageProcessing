# **Journey of 66DaysofData in Natural Language Processing**

**Day1 of 66DaysOfData!**
- Natural Language Processing is a field of Linguistics, Computer Science, and Artificial Intelligence concerned with the interactions between Computers and Human language, in particular how to program computers to process and analyze large amounts of Natural Language Data. Today, I am learning NLP from very first beginning. Here, I have provided the short description of various Libraries, Dependencies and Modules required for Natural Language Processing. I have also performed the Processing of Text such as Removing the Retweet Text, Removing the Hyperlinks and Removing Hashtags. Excited about the days ahead!!

![Image](https://github.com/ThinamXx/66DaysofData__NLP/blob/master/Images/Day%201.PNG)

**Day2 of 66DaysOfData!**
- In Natural Language Processing, String Tokenization is a process where the string is split into individual words or individual parts without blanks and tabs. In the same step, the words in the string is converted into lower case. The Tokenize module from NLTK(Naural Language Toolkit) makes very easy to carry out this process. In my journey of Natural Language Processing, Today I learned about String Tokenization, Stop word and Punctuation in NLP. I have implemented TweetTokenizer and performed the process to remove stopwords and punctuation from the tokenized tweets. Excited about the days ahead!!

![Image](https://github.com/ThinamXx/66DaysofData__NLP/blob/master/Images/Day%202.PNG)

**Day3 of 66DaysOfData!**
- Stemming in Natural Language Processing: Stemming is a process of converting a word to its most general form or stem. It's basically the process of removing the suffix from a word and reduce it to its root word. It helps in reducing the size of vocabulary. In my journey of Natural Language Processing, Today I learned about Stemming in NLP which is one of the most important steps while working with text. I have presented the implementation of Porter Stemmer, Snowball Stemmer and Lancaster Stemmer. Excited for coming days!!
  - Porter Stemmer is one of the most common and gentle stemmer which is very fast but not very precise.
  - Snowball Stemmer, whose actual name is English Stemmer is more precise over large data-sets.
  - Lancaster Stemmer is very aggressive algorithm. It will hugely trim down the working data which itself has pros and cons.
  
![Image](https://github.com/ThinamXx/66DaysofData__NLP/blob/master/Images/Day%203.PNG)
  
**Day4 of 66DaysOfData!**
- Lemmatization in Natural Language Processing: Lemmatization is the process of grouping together the inflected forms of words so that they can analysed as a single item, identified by the word's lemma or a dictionary form. It is the process where individual tokens from a sentence or words are reduced to their base form. Lemmatization is much more informative than simple stemming. Lemmatization looks at the surrounding text to determine a given words's part of speech where it doesn't categorize the phrases. In my journey of Natural Language Processing, Today I learned about Lemmatization and it's simple implementation using Spacy as well NLTK. I have covered the fundamentals of NLP such as Tokenization, Stemming and Lemmatization. Excited for the days ahead!!
  
![Image](https://github.com/ThinamXx/66DaysofData__NLP/blob/master/Images/Day%204.PNG)

**Day5 of 66DaysOfData!**
- Natural Language Processing is a field of Linguistics, Computer Science, and Artificial Intelligence concerned with the interactions between Computers and Human language, in particular how to program computers to process and analyze large amounts of Natural Language Data. As a part of Natural Langauge Processing journey, I have started the book, [Natural Language Processing with Python](https://www.nltk.org/book/). It's really amazing and I have encountered some very basic functions but unknown to me such as concordance function, similar function, common context function and a basic dispersion plot as well. I will be using this book in my journey. Excited for the days ahead!!
- [**Natural Language Processing with Python :)**](https://www.nltk.org/book/)

**Day6 of 66DaysOfData!**
- In my journey of Natural Language Processing, Today I have explored about Gutenberg Corpus using NLTK and Python. I have also learned about various interesting challenges with proper explanation of each topic under the hood of NLP such as:
  - Word Sense Disambiguation: In NLP, Word Sense Disambiguation is an open problem concerned with identifying which sense of a word is used in a sentence. The solution to this issue impacts other Computer related writing such as discourse, improving relevance of Search Engines, Anaphora Resolution, Coherence, and Inference.
  - Pronounce Resolution
  - Generating Language Output
  - Machine Translation: Machine Translation is a sub field of Computational Linguistics that investigates the use of software to translate text or speech from one language to another.
  - Spoken Dialog System
  - Textual Entailment

![Image](https://github.com/ThinamXx/66DaysofData__NLP/blob/master/Images/Day%206.PNG)

**Day7 of 66DaysOfData!**
- In my journey of Natural Language Processing, Today I have learned about different Text Corpora and Basic Corpus Functionality defined in NLTK. I have also learned about Loading own Corpus, Plotting and Tabulating Distributions and Generating Random Text with Bigrams. The topics I have covered are summarized below:
  - Web and Chat Text
  - Brown Corpus
  - Reuters Corpus
  - Inaugural Address Corpus
  - Annotated Text Corpora
  - Corpora in Other Languages
  - Conditional Frequency Distributions

![Image](https://github.com/ThinamXx/66DaysofData__NLP/blob/master/Images/Day%207.PNG)

**Day8 of 66DaysOfData!**
- In my journey of Natural Language Processing, Today I am learning about Processing Raw Text in Natural Language Processing. Basically, I have completed processing the Text from Electronic Books and from HTML documents. Apart from that, I have learned about WordNet. The topics I have covered in WordNet are:
  - The WordNet Hierarchy and
  - Semantic Similarity: Semantic Similarity is a metric defined over a set of Documents or Terms where the idea of distance between items is based on the likeness of their meaning or semantic content as opposed to Lexicographical Similarity. Example:"Road" and "Driving".

![Image](https://github.com/ThinamXx/66DaysofData__NLP/blob/master/Images/Day%208.PNG)

**Day9 of 66DaysOfData!**
- In my journey of Natural Language Processing, Today I have learned about Text Processing with Unicode and Regular Expressions for Detecting Word Patterns with proper explanations. Within a Program, we can manipulate Unicode just like normal strings. Unicode are encoded as stream of bytes. Examples: ASCII, Latin-2, UTF-8. I have also implemented the Regular Expressions for Detecting the Word Patterns using basic meta characters such as:
  - Dollar sign ( $ ) matches the characters of the end of word.
  - Dot symbol ( . ) matches any single character.
  - Caret symbol ( ^ ) matches the characters of the start of word.
  - Question mark ( ? ) specifies the previous character is optional.
  - Plus sign ( + ) means one or more instances of the preceding item.
  - Sign ( * ) means zero or more instances of the preceding item.
  - Backslash ( \ ) means following character is deprived.
  - Pipe symbol ( | ) specifies the choice between left and right.

![Image](https://github.com/ThinamXx/66DaysofData__NLP/blob/master/Images/Day%209.PNG)

**Day10 of 66DaysOfData!**
- In my journey of Natural Language Processing, Today I have learned about the various useful applications of Regular Expressions such as Finding Word Stems, Regular Expressions for Tokenizing Text, Normalizing Text such as Stemmers and Lemmatization. I have also read about the issues of Tokenization: Tokenization turns out to be far more difficult task than one might have expected. No single solution works well accross the board. Another issue of Tokenization is the presence of contractions as well. Example: didn't.

![Image](https://github.com/ThinamXx/66DaysofData__NLP/blob/master/Images/Day%2010.PNG)

**Day11 of 66DaysOfData!**
- In my journey of Natural Language Processing, Today I have completed all the preliminaries process or techniques required in Natural Language Processing included in the book, "Natural Language Processing with Python". I have completed the topics such as Tokenization, Stemming, Lemmatization and Text Processing with Regular Expressions. Apart from that, I have also worked with various Text Corpora such as Brown Corpus, Inaugural Address Corpus, Reuters Corpus and so on. And I have completed first 110 pages of the book, Natural Language Processing with Python".

![Image](https://github.com/ThinamXx/66DaysofData__NLP/blob/master/Images/Day%2011%20a.PNG)

- I have plotted a simple bar plot using various categories of Brown Corpus presenting the Frequency Distribution of various words appearing inside the Corpus. I hope you can gain insights from the plot as well. Excited about the days ahead !!

![Image](https://github.com/ThinamXx/66DaysofData__NLP/blob/master/Images/Day%2011.PNG)
