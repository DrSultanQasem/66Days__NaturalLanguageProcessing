# **Journey of 66DaysofData in Natural Language Processing**

**Day1 of 66DaysOfData!**
- Natural Language Processing is a field of Linguistics, Computer Science, and Artificial Intelligence concerned with the interactions between Computers and Human language, in particular how to program computers to process and analyze large amounts of Natural Language Data. Today, I am learning NLP from very first beginning. Here, I have provided the short description of various Libraries, Dependencies and Modules required for Natural Language Processing. I have also performed the Processing of Text such as Removing the Retweet Text, Removing the Hyperlinks and Removing Hashtags. Excited about the days ahead!!

![Image](https://github.com/ThinamXx/66DaysofData__NLP/blob/master/Images/Day%201.PNG)

**Day2 of 66DaysOfData!**
- In Natural Language Processing, String Tokenization is a process where the string is split into individual words or individual parts without blanks and tabs. In the same step, the words in the string is converted into lower case. The Tokenize module from NLTK(Naural Language Toolkit) makes very easy to carry out this process. In my journey of Natural Language Processing, Today I learned about String Tokenization, Stop word and Punctuation in NLP. I have implemented TweetTokenizer and performed the process to remove stopwords and punctuation from the tokenized tweets. Excited about the days ahead!!

![Image](https://github.com/ThinamXx/66DaysofData__NLP/blob/master/Images/Day%202.PNG)

**Day3 of 66DaysOfData!**
- Stemming in Natural Language Processing: Stemming is a process of converting a word to its most general form or stem. It's basically the process of removing the suffix from a word and reduce it to its root word. It helps in reducing the size of vocabulary. In my journey of Natural Language Processing, Today I learned about Stemming in NLP which is one of the most important steps while working with text. I have presented the implementation of Porter Stemmer, Snowball Stemmer and Lancaster Stemmer. Excited for coming days!!
  - Porter Stemmer is one of the most common and gentle stemmer which is very fast but not very precise.
  - Snowball Stemmer, whose actual name is English Stemmer is more precise over large data-sets.
  - Lancaster Stemmer is very aggressive algorithm. It will hugely trim down the working data which itself has pros and cons.
  
![Image](https://github.com/ThinamXx/66DaysofData__NLP/blob/master/Images/Day%203.PNG)
  
**Day4 of 66DaysOfData!**
- Lemmatization in Natural Language Processing: Lemmatization is the process of grouping together the inflected forms of words so that they can analysed as a single item, identified by the word's lemma or a dictionary form. It is the process where individual tokens from a sentence or words are reduced to their base form. Lemmatization is much more informative than simple stemming. Lemmatization looks at the surrounding text to determine a given words's part of speech where it doesn't categorize the phrases. In my journey of Natural Language Processing, Today I learned about Lemmatization and it's simple implementation using Spacy as well NLTK. I have covered the fundamentals of NLP such as Tokenization, Stemming and Lemmatization. Excited for the days ahead!!
  
![Image](https://github.com/ThinamXx/66DaysofData__NLP/blob/master/Images/Day%204.PNG)

**Day5 of 66DaysOfData!**
- Natural Language Processing is a field of Linguistics, Computer Science, and Artificial Intelligence concerned with the interactions between Computers and Human language, in particular how to program computers to process and analyze large amounts of Natural Language Data. As a part of Natural Langauge Processing journey, I have started the book, [Natural Language Processing with Python](https://www.nltk.org/book/). It's really amazing and I have encountered some very basic functions but unknown to me such as concordance function, similar function, common context function and a basic dispersion plot as well. I will be using this book in my journey. Excited for the days ahead!!
- [**Natural Language Processing with Python :)**](https://www.nltk.org/book/)

**Day6 of 66DaysOfData!**
- In my journey of Natural Language Processing, Today I have explored about Gutenberg Corpus using NLTK and Python. I have also learned about various interesting challenges with proper explanation of each topic under the hood of NLP such as:
  - Word Sense Disambiguation: In NLP, Word Sense Disambiguation is an open problem concerned with identifying which sense of a word is used in a sentence. The solution to this issue impacts other Computer related writing such as discourse, improving relevance of Search Engines, Anaphora Resolution, Coherence, and Inference.
  - Pronounce Resolution
  - Generating Language Output
  - Machine Translation: Machine Translation is a sub field of Computational Linguistics that investigates the use of software to translate text or speech from one language to another.
  - Spoken Dialog System
  - Textual Entailment

![Image](https://github.com/ThinamXx/66DaysofData__NLP/blob/master/Images/Day%206.PNG)

**Day7 of 66DaysOfData!**
- In my journey of Natural Language Processing, Today I have learned about different Text Corpora and Basic Corpus Functionality defined in NLTK. I have also learned about Loading own Corpus, Plotting and Tabulating Distributions and Generating Random Text with Bigrams. The topics I have covered are summarized below:
  - Web and Chat Text
  - Brown Corpus
  - Reuters Corpus
  - Inaugural Address Corpus
  - Annotated Text Corpora
  - Corpora in Other Languages
  - Conditional Frequency Distributions

![Image](https://github.com/ThinamXx/66DaysofData__NLP/blob/master/Images/Day%207.PNG)

**Day8 of 66DaysOfData!**
- In my journey of Natural Language Processing, Today I am learning about Processing Raw Text in Natural Language Processing. Basically, I have completed processing the Text from Electronic Books and from HTML documents. Apart from that, I have learned about WordNet. The topics I have covered in WordNet are:
  - The WordNet Hierarchy and
  - Semantic Similarity: Semantic Similarity is a metric defined over a set of Documents or Terms where the idea of distance between items is based on the likeness of their meaning or semantic content as opposed to Lexicographical Similarity. Example:"Road" and "Driving".

![Image](https://github.com/ThinamXx/66DaysofData__NLP/blob/master/Images/Day%208.PNG)

**Day9 of 66DaysOfData!**
- In my journey of Natural Language Processing, Today I have learned about Text Processing with Unicode and Regular Expressions for Detecting Word Patterns with proper explanations. Within a Program, we can manipulate Unicode just like normal strings. Unicode are encoded as stream of bytes. Examples: ASCII, Latin-2, UTF-8. I have also implemented the Regular Expressions for Detecting the Word Patterns using basic meta characters such as:
  - Dollar sign ( $ ) matches the characters of the end of word.
  - Dot symbol ( . ) matches any single character.
  - Caret symbol ( ^ ) matches the characters of the start of word.
  - Question mark ( ? ) specifies the previous character is optional.
  - Plus sign ( + ) means one or more instances of the preceding item.
  - Sign ( * ) means zero or more instances of the preceding item.
  - Backslash ( \ ) means following character is deprived.
  - Pipe symbol ( | ) specifies the choice between left and right.

![Image](https://github.com/ThinamXx/66DaysofData__NLP/blob/master/Images/Day%209.PNG)

**Day10 of 66DaysOfData!**
- In my journey of Natural Language Processing, Today I have learned about the various useful applications of Regular Expressions such as Finding Word Stems, Regular Expressions for Tokenizing Text, Normalizing Text such as Stemmers and Lemmatization. I have also read about the issues of Tokenization: Tokenization turns out to be far more difficult task than one might have expected. No single solution works well accross the board. Another issue of Tokenization is the presence of contractions as well. Example: didn't.

![Image](https://github.com/ThinamXx/66DaysofData__NLP/blob/master/Images/Day%2010.PNG)

**Day11 of 66DaysOfData!**
- In my journey of Natural Language Processing, Today I have completed all the preliminaries process or techniques required in Natural Language Processing included in the book, "Natural Language Processing with Python". I have completed the topics such as Tokenization, Stemming, Lemmatization and Text Processing with Regular Expressions. Apart from that, I have also worked with various Text Corpora such as Brown Corpus, Inaugural Address Corpus, Reuters Corpus and so on. And I have completed first 110 pages of the book, "Natural Language Processing with Python".

![Image](https://github.com/ThinamXx/66DaysofData__NLP/blob/master/Images/Day%2011%20a.PNG)

- I have plotted a simple bar plot using various categories of Brown Corpus presenting the Frequency Distribution of various words appearing inside the Corpus. I hope you can gain insights from the plot as well. Excited about the days ahead !!

![Image](https://github.com/ThinamXx/66DaysofData__NLP/blob/master/Images/Day%2011.PNG)

**Day12 of 66DaysOfData!**
- Part of Speech Tagging: The process of classifying words into their "Parts of Speech" and Labeling them accordingly is known as Part of Speech Tagging which is also known as POS tagging or simply Tagging. The collections of tags used for a particular task is known as Tagset. In my journey of Natural Language Processing, Today I have learned about Automatic Tagging such as Default Tagger, Regular Expression Tagger and Lookup Tagger along with N-Gram Tagging such as Unigram Tagger, Bigram Tagger and so on. I have also learned about Combining Taggers using backoff parameter. I hope you can gain insights about the Implementation of Tagging. Excited about the days to come !!

![Image](https://github.com/ThinamXx/66DaysofData__NLP/blob/master/Images/Day%2012.PNG)

**Day13 of 66DaysOfData!**
- Supervised Classification: Classification is the process of choosing the correct class label for a given input. The Classification is said to be Supervised Classification if it is built based on training corpora containing the correct label for each input. One common example of Classification is deciding whether an Email is spam or not. In my journey of Natural Language Processing, Today I have learned about Supervised Classification. I have covered the topics such as Choosing Right Features, Document Classification, Gender Identification, Part of Speech Tagging using Naive Bayes Classifier and Decision Trees Classifier under the hood of Supervised Classification of NLP. I have presented the basic Implementation of Naive Bayes Classifier in Document Classification and I hope you can gain insights from it as well. Excited about the days ahead!!

![Image](https://github.com/ThinamXx/66DaysofData__NLP/blob/master/Images/Day%2013.PNG)

**Day14 of 66DaysOfData!**
- In my journey of Natural Language Processing, Today I have learned about Sequence Classification, Sentence Segmentation and various Evaluation methods under the hood of Natural Language Processing. I have covered the fundamental topics such as Test Data, Accuracy measure, Precision and Recall, Confusion Matrices, Cross validation, Decision Trees and Naive Bayes with proper implementations which are so helpful for my understanding. I have been in this journey for 2 weeks and I have covered all the fundamentals which are so relevant in Natural Language Processing. I had been following the "Natural Language Processing with Python" book and it really helps me a lot. Now, I will be focusing more on Implementations so that I will be following the course of Fastai on Natural Language Processing. I have implemented the Naive Bayes Classifier in Text Corpus and I hope you can gain insight about the Implementation of Naive Bayes. Excited about the days ahead!!

![Image](https://github.com/ThinamXx/66DaysofData__NLP/blob/master/Images/Day%2014%20a.PNG)

**Day15 of 66DaysOfData!**
- Singular Value Decomposition (SVD): The words that appear most frequently in one topic would appear less frequently in the other, otherwise that word wouldn't make a good choice to separate out the two topics. Therefore, the topics are Orthogonal. The SVD algorithm factorizes a matrix into one matrix with orthogonal columns and one with orthogonal rows along with diagonal matrix which contains the relative importance of each factor.
- NonNegative Matrix Factorization (NMF): Non Negative Matrix Factorization (NMF) is a factorization or constrain of non negative dataset. NMF is non exact factorization that factors into one short positive matrix.
- Topic Frequency Inverse Document Frequency (TFIDF): TFIDF is a way to normalize the term counts by taking into account how often they appear in a document and how long the document is and how common or rare the document is.
- In my journey of Natural Language Processing, Today I have learned and implemented SVD, NMF and TFIDF in Topic Modeling project. I have captured just the overview of the implementations here. I hope you can gain insights.
- [**Topic Modeling with SVD and NMF**](https://github.com/ThinamXx/NaturalLanguageProcessing_NLP/blob/master/Topic%20Modeling%20with%20SVD%20and%20NMF.ipynb)

![Image](https://github.com/ThinamXx/66DaysofData__NLP/blob/master/Images/Day%2015.PNG)
![Image](https://github.com/ThinamXx/66DaysofData__NLP/blob/master/Images/Day%2015%20a.PNG)

**Day16 of 66DaysOfData!**
- In my journey of Natural Language Processing, Today I have learned the Implementations of NLP from Fastai course which has been published recently. As the Fastai course primarily focuses on the Coding part and follows the top down aspect of Learning and Teaching. It's bit complicated to learn than other courses. Fastai's API is really amazing and powerful as well. I learned the basic steps of NLP with Fastai such as Word Tokenization, Subword Tokenization, Numericalization, and Preparing TextBlock and DataBlock. I am currently working on Sentiment Analysis of IMDB reviews using Fastai. I have shared the Implementations of the Word Tokenization, Subword Tokenization, Numericalization and process to prepare the TextBlock and DataBlock with Fastai here. I hope you can gain insights about the Implementation of Fastai from here. Excited about the days ahead!!

![Image](https://github.com/ThinamXx/66DaysofData__NLP/blob/master/Images/Day%2016.PNG)

**Day17 of 66DaysOfData!**
- In my journey of Natural Language Processing, Today I have learned about the Implementation of Fastai in preparing a Sentiment Classifier Model. I have prepared a Model using Fastai API which can classify Sentiment of Internet Movie Database reviews i.e. classifying the Positive or Negative Sentiment. Fastai's API is really powerful and effective so that the Model can classify the Sentiment of Internet Movie Database reviews with above 90% accuracy in just few lines of code. I have learned about Word Tokenization, Subword Tokenization, Numericalization, TextBlock and DataBlock API and Training the Classifier Model using Fastai.I have presented the snapshot of the Implementation of Fastai API in preparing the Language Model and Training the Model. I have also presented the Implementation of Fastai API in preparing the Classifier Model using the Language Model and also the concept of unfreezing the Model. I hope you can gain insights from here as well. This snapshot is the continuation of yesterday's snapshot. Excited about the days ahead!!
- [**Sentiment Classification of Internet Movie Database reviews**](https://github.com/ThinamXx/InternetMovieDatabase__NLP/blob/master/IMDB.ipynb)

![Image](https://github.com/ThinamXx/66DaysofData__NLP/blob/master/Images/Day%2017.PNG)

**Day18 of 66DaysOfData!**
- In my journey of Natural Language Processing, Today I have read a topic of Natural Language Processing in the book [**Dive into Deep Learning**](https://d2l.ai/) by Aston Zhang. Here, I have covered the topics such as Text Processing, Machine Translation, Natural Language Processing pre training and Word Embedding. The information and explanations were great and the code implementation is in MXNET. I am not quite familiar with MXNET framework so I have just presented a small snapshot here. Apart from that, Today I have read a very small part of the book [**Natural Language Processing in Action**](https://g.co/kgs/U82Sem) by Hobson Lane. I am so much thankful to Anthony Camarillo for sharing this book with me. And I will give continuation with this book [**Natural Language Processing in Action**](https://g.co/kgs/U82Sem). Excited about the days ahead!!

![Image](https://github.com/ThinamXx/66DaysofData__NLP/blob/master/Images/Day%2018.PNG)

**Day19 of 66DaysOfCode!**
- In my journey of Natural Language Processing, Today I have read and implemented the first chapter of the book, [Natural Language Processing in Action](https://g.co/kgs/U82Sem). In this chapter, I have covered the topics such as Natural Language Processing and the Magic, Regular Expressions, Finite State Machine (FSM) concept, Word order and Grammar, simple NLP Chatbot pipeline and Natural Language IQ. I have presented the simple Chatbot using Regular Expressions and Finite State Machine (FSM) concept. Basically, I will be working on much advanced Chatbots using Neural Networks in coming days. I hope you will also google out the FSM concept in NLP and also the Implementation of Regular Expressions in FSM from here. Excited about the days ahead!!

![Image](https://github.com/ThinamXx/66DaysofData__NLP/blob/master/Images/Day%2019.PNG)

**Day20 of 66DaysOfData!**
- In my journey of Natural Language Processing, Today I have read a couple of topics under the hood of Natural Language Processing such as Tokenized Phrase, Dot Product in NLP, Bag of Words and Overlapping, Token improvement with Regex which is apart from Regular Expressions, Contractions, Extending Vocabulary with NGrams. I have also read and implemented the Regex Tokenizer, Tree Bank Tokenizer and Casual Tokenizer. Actually I am continuing my learning journey with a book [Natural Language Processing in Action](https://g.co/kgs/U82Sem) and lots of the preprocessing concepts which I have already read are coming along my way. I prefer to go through the concepts again because I don't want to skip any topics from this book. Although the concepts might match along the way I won't repeat the same implementation in any of my Snapshots. I have presented the Implementation of Regex Tokenizer, Tree Bank Tokenizer, Casual Tokenizer and NGrams here in the Snapshots. These Tokenization steps are more better than Traditional Tokenization steps using Regular Expressions. I hope you will spend some time to learn about these Tokenizers. Excited about the days ahead!!

![Image](https://github.com/ThinamXx/66DaysofData__NLP/blob/master/Images/Day%2020.PNG)

**Day21 of 66DaysOfData!**
- In my journey of Natural Language Processing, Today I have read and implemented about Stopwords, Stemming, Lemmatization and Sentiment Analysis using VADER Approach which is the Algorithm composed by human and also the Machine Learning Approach with the help of Naive Bayes Classifier. I have completed the first 2 chapters of the book, [Natural Language Processing in Action](https://g.co/kgs/U82Sem) and it is helping me a lot along my journey. I have presented the Implementation of VADER Approach in Sentiment Analysis along with Naive Bayes Classifier and I have also included the Implementation of Casual Tokenizer in the Movies Dataset. I hope you will gain insights about the Implementation of VADER Approach and Naive Bayes in Sentiment Analysis. Actually, VADER Approach is not as efficient as Machine Learning Approach such as Naive Bayes Classifier. I hope you will spend some time in learning about Naive Bayes in Sentiment Analysis along with VADER Approach. Excited about the days to come!!

![Image](https://github.com/ThinamXx/66DaysofData__NLP/blob/master/Images/Day%2021.PNG)

**Day22 of 66DaysOfData!**
- In my journey of Natural Language Processing, Today I have read and Implemented about Bag of Words, Vectorizing concept, Vector Spaces, Cosine Similarity, Zipf's Law and Inverse Frequency concept, Text Modeling, TFIDF, Relevance Ranking and Okapi BM25 concept. I have completed the first three chapters of the book [Natural Language Processing in Action](https://g.co/kgs/U82Sem) and this chapter primarily focuses on the concept of Vectorizing the Tokens which are obtained after Tokenization using TFIDF Vectorizer. Text Vectorization is the process of converting Text into Numerical representation. I have also read and Implemented the concept of Cosine Similarity under the hood of Natural Language Processing. I have presented the Implementation of TFIDF Vectorizer and also the process of Tokenizing the Text Documents and removing the Stopwords. I have also Implemented the Cosine Similarity using Numpy and pure Python as well in this Snapshot. I hope you will gain insights about Text Vectorization and Tokenization from here. Excited about the days to come!!

![Image](https://github.com/ThinamXx/66DaysofData__NLP/blob/master/Images/Day%2022.PNG)

**Day23 of 66DaysOfData!**
- In my journey of Natural Language Processing, Today I have started learning about Semantic Analysis. I have read and implemented about Topic Vectors, Algorithms for Scoring Topics and Semantic Analysis such as Latent Semantic Analysis **LSA**, Linear Discriminant Analysis **LDA** and Latent Dirichlet Allocation **LDIA**. Today, I primarily focused on reading and Implementing about Linear Discriminant Analysis **LDA**. LDA is one of the most straight forward and fast Dimension Reduction and Classification Models. In Natural Language Processing, Semantic Analysis is the process of relating the Syntactic structures from the levels of Phrases, Clauses, Sentences and Paragraphs to the level of the writing as a whole and to their Language dependent meanings. I have presented the Implementation of LDA's working Principal which states that Computing the centroid of TFIDF vectors for each side of the binary Class here in the Snapshot. I hope you will gain insights about Implementation of LDA Classifier and creating NLP Pipeline of Tokenizer and Vectorizer from here. And I hope you will spend some time in learning about Semantic Analysis as well. Excited about the days ahead !!

![Image](https://github.com/ThinamXx/66DaysofData__NLP/blob/master/Images/Day%2023.PNG)

**Day24 of 66DaysOfData!**
- In my journey of Natural Language Processing, Today I have read and Implemented Latent Semantic Analysis "LSA", Singular Value Decomposition "SVD", Principal Component Analysis "PCA", Truncated SVD and Latent Dirichlet Allocation "LDIA". I have primarily focused on reading and Implementing LSA and LDIA for Semantic Analysis. LSA works well with Normalized TFIDF Vectors whereas LDIA works well with raw Bag of Words "BOW" Count Vectors. Semantic Analysis is the process of relating the Syntactic structures from the levels of Phrases, Clauses, Sentences and Paragraphs to the level of the writing as a whole and to their Language dependent meanings. I have presented the Implementation of Linear Discriminant Analysis "LDA" while working with TFIDF Vectors and BOW Count Vectors here in the Snapshot. I hope you will gain insights about the Implementation of LDA Classifier along with LDIA Topic Vectors and BOW count Vectors. Excited about the days to come!! Incase you want to see my Notebook, I have presented the overall Implementation of Semantic Analysis with LSA, LDA and LDIA with proper Documentation here: [**Semantic Analysis**](https://github.com/ThinamXx/SemanticAnalysis__NLP.git)

![Image](https://github.com/ThinamXx/66Days__NaturalLanguageProcessing/blob/master/Images/Day%2024.PNG)
